# -*- coding: utf-8 -*-
"""HUMAN_ACTIVITY_RECOGNITION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MxVaWtM67rx5JHMOSFjRtrP3wQnEHsIm

# IMPORTING LIBRARIES
"""

from __future__ import print_function
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from scipy import stats
from IPython.display import display, HTML

import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
import os
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV, ShuffleSplit, train_test_split
from keras.models import Sequential
import itertools
from sklearn.metrics import confusion_matrix
from keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn import preprocessing

import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Reshape
from keras.layers import Conv2D, MaxPooling2D
from keras.utils import np_utils
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV

"""**DEFINING LABELS**"""

LABELS = ['Downstairs',
          'Jogging',
          'Sitting',
          'Standing',
          'Upstairs',
          'Walking']

"""**DEFINING TIME PERIOD AND STEP SIZE**"""

TIME_PERIODS = 80
STEP_DISTANCE = 40

#We can define time period as the number of steps within one segment
#Step_distance identifies the number of steps to be taken from one segment to the next segment
#If time_periods = step_distance then there is no overlap between the segments

#Defining some standards
pd.options.display.float_format = '{:.1f}'.format
sns.set() # Default seaborn look and feel
plt.style.use('ggplot')

"""**METHOD TO CONVERT TO FLOAT**"""

def convert_to_float(x):

    try:
        return np.float(x)
    except:
        return np.nan

"""**METHOD TO READ DATA FROM A FILE**"""

def read_data(file_path):

    column_names = ['user-id',
                    'activity',
                    'timestamp',
                    'x-axis',
                    'y-axis',
                    'z-axis']
    df = pd.read_csv(file_path,
                     header=None,
                     names=column_names)
    # Last column has a ";" character which must be removed ...
    df['z-axis'].replace(regex=True,
      inplace=True,
      to_replace=r';',
      value=r'')
    # ... and then this column must be transformed to float explicitly
    df['z-axis'] = df['z-axis'].apply(convert_to_float)
    # This is very important otherwise the model will not fit and loss
    # will show up as NAN
    df.dropna(axis=0, how='any', inplace=True)

    return df

"""**METHOD TO DISPLAY DATAFRAME INFO**"""

def show_basic_dataframe_info(dataframe):

    # Shape and how many rows and columns
    print('Number of columns in the dataframe: %i' % (dataframe.shape[1]))
    print('Number of rows in the dataframe: %i\n' % (dataframe.shape[0]))

"""# **LOADING WISDM DATASET**"""

# Load data set containing all the data from csv
df = read_data('/content/drive/MyDrive/wisdm_ar_latest/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt')

# Describe the data
show_basic_dataframe_info(df)
df.head(30)

"""# PLOTTING INFORMATION"""

def plot_activity(activity, data):

    fig, (axis_x, axis_y, axis_z) = plt.subplots(nrows=3,
         figsize=(15, 10),
         sharex=True)
    plot_axis(axis_x, data['timestamp'], data['x-axis'], 'X-Axis')
    plot_axis(axis_y, data['timestamp'], data['y-axis'], 'Y-Axis')
    plot_axis(axis_z, data['timestamp'], data['z-axis'], 'Z-Axis')
    plt.subplots_adjust(hspace=0.2)
    fig.suptitle(activity)
    plt.subplots_adjust(top=0.90)
    plt.show()

def plot_axis(ax, x, y, title):

    ax.plot(x, y, 'r')
    ax.set_title(title)
    ax.xaxis.set_visible(False)
    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])
    ax.set_xlim([min(x), max(x)])
    ax.grid(True)

# Number of training examples which exist for each of the six activities
df['activity'].value_counts().plot(kind='bar',color='green',x = 'Activity',y='Training examples',title='Traning examples for each activity')
plt.show()

# Better understand how the recordings are spread across the different
# users who participated in the study
df['user-id'].value_counts().plot(kind='bar',color ='gray',
                                  title='Training Examples by User')
plt.show()

"""**Plotting for each activity**"""

for activity in np.unique(df['activity']):
    subset = df[df['activity'] == activity][:180]
    plot_activity(activity, subset)

# Define column name of the label vector
LABEL = 'ActivityEncoded'
# Transform the labels from String to Integer via LabelEncoder
le = preprocessing.LabelEncoder()
# Add a new column to the existing DataFrame with the encoded values
df[LABEL] = le.fit_transform(df['activity'].values.ravel())

"""**DATASET SPLITTING**"""

df_test = df[df['user-id'] > 28]
df_train = df[df['user-id'] <= 28]

"""**Normalizing features**"""

# Normalize features for training data set (values between 0 and 1)
# Surpress warning for next 3 operation
pd.options.mode.chained_assignment = None  # default='warn'
df_train['x-axis'] = df_train['x-axis'] / df_train['x-axis'].max()
df_train['y-axis'] = df_train['y-axis'] / df_train['y-axis'].max()
df_train['z-axis'] = df_train['z-axis'] / df_train['z-axis'].max()
# Round numbers
df_train = df_train.round({'x-axis': 4, 'y-axis': 4, 'z-axis': 4})

def create_segments_and_labels(df, time_steps, step, label_name):

    # x, y, z acceleration as features
    N_FEATURES = 3
    # Number of steps to advance in each iteration (for me, it should always
    # be equal to the time_steps in order to have no overlap between segments)
    # step = time_steps
    segments = []
    labels = []
    for i in range(0, len(df) - time_steps, step):
        xs = df['x-axis'].values[i: i + time_steps]
        ys = df['y-axis'].values[i: i + time_steps]
        zs = df['z-axis'].values[i: i + time_steps]
        # Retrieve the most often used label in this segment
        label = stats.mode(df[label_name][i: i + time_steps])[0][0]
        segments.append([xs, ys, zs])
        labels.append(label)

    # Bring the segments into a better shape
    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, time_steps, N_FEATURES)
    labels = np.asarray(labels)

    return reshaped_segments, labels

# Normalize features for training data set (values between 0 and 1)
# Surpress warning for next 3 operation
pd.options.mode.chained_assignment = None  # default='warn'
df_train['x-axis'] = df_train['x-axis'] / df_train['x-axis'].max()
df_train['y-axis'] = df_train['y-axis'] / df_train['y-axis'].max()
df_train['z-axis'] = df_train['z-axis'] / df_train['z-axis'].max()
# Round numbers
df_train = df_train.round({'x-axis': 4, 'y-axis': 4, 'z-axis': 4})

x_train, y_train = create_segments_and_labels(df_train,
                                              TIME_PERIODS,
                                              STEP_DISTANCE,
                                              LABEL)

print('x_train shape: ', x_train.shape)
print(x_train.shape[0], 'training samples')
print('y_train shape: ', y_train.shape)

# Set input & output dimensions
num_time_periods, num_sensors = x_train.shape[1], x_train.shape[2]
num_classes = le.classes_.size
print(list(le.classes_))

input_shape = (num_time_periods*num_sensors)
x_train = x_train.reshape(x_train.shape[0], input_shape)
print('x_train shape:', x_train.shape)
print('input_shape:', input_shape)

x_train = x_train.astype('float32')
y_train = y_train.astype('float32')

#one hot encoding on train data

y_train_hot = np_utils.to_categorical(y_train, num_classes)
print('New y_train shape: ', y_train_hot.shape)

# Normalize features for training data set
df_test['x-axis'] = df_test['x-axis'] / df_test['x-axis'].max()
df_test['y-axis'] = df_test['y-axis'] / df_test['y-axis'].max()
df_test['z-axis'] = df_test['z-axis'] / df_test['z-axis'].max()

df_test = df_test.round({'x-axis': 4, 'y-axis': 4, 'z-axis': 4})

x_test, y_test = create_segments_and_labels(df_test,
                                            TIME_PERIODS,
                                            STEP_DISTANCE,
                                            LABEL)

# Set input_shape / reshape for Keras
x_test = x_test.reshape(x_test.shape[0], input_shape)

x_test = x_test.astype('float32')
y_test = y_test.astype('float32')

y_test = np_utils.to_categorical(y_test, num_classes)

"""# **MODEL**

# **HYPERPARAMETER TUNING**
"""

batch_size = [512,256]
epoch = [40,50]

max_acc_cnn         = 0
max_valid_acc_cnn   = 0
min_loss            = 0 
min_valid_loss      = 0
best_batch_size     = 0
best_epoch          = 0

for b in batch_size:
  for e in epoch:
    model_m = tf.keras.models.Sequential()
    # Remark: since coreml cannot accept vector shapes of complex shape like
    # [80,3] this workaround is used in order to reshape the vector internally
    # prior feeding it into the network
    model_m.add(Reshape((TIME_PERIODS, 3), input_shape=(input_shape,)))
    model_m.add(Dense(150, activation='relu'))
    model_m.add(Dropout(0.2))
    model_m.add(Dense(140, activation='relu'))
    model_m.add(Dropout(0.2))
    model_m.add(Dense(130, activation='relu'))
    model_m.add(Dropout(0.2))
    model_m.add(Dense(120, activation='relu'))
    model_m.add(Dropout(0.2))
    model_m.add(Dense(100, activation='relu'))
    model_m.add(Dropout(0.2))
    model_m.add(Dense(80, activation='relu'))
    model_m.add(Flatten())
    model_m.add(Dense(num_classes, activation='softmax'))

    model_m.compile(loss=keras.losses.categorical_crossentropy,optimizer='adam', metrics=['accuracy']) 


    i_model = model_m.fit(x_train, y_train_hot, epochs = e , batch_size = b , validation_split = 0.2 , verbose = 1)

    i_acc_cnn         = np.mean(i_model.history['accuracy'])
    i_valid_acc_cnn   = np.mean(i_model.history['val_accuracy'])
    i_loss            = np.mean(i_model.history['loss'])
    i_valid_loss      = np.mean(i_model.history['val_loss'])

    if(i_valid_acc_cnn > max_valid_acc_cnn):
      if(min_valid_loss < i_valid_loss):
        max_acc_cnn         = i_acc_cnn
        max_valid_acc_cnn   = i_valid_acc_cnn
        min_loss            = i_loss
        min_valid_loss      = i_valid_loss
        best_batch_size     = b
        best_epoch  = e

print("maximum accuracy = ",max_acc_cnn)
print("\n maximum accuracy on validation data = ",max_valid_acc_cnn)
print("\n minimum loss = ",min_loss) 
print("\n minimum loss on validation data =",min_valid_loss)
print("\n batch size = ",best_batch_size)
print("\n total epochs = ",best_epoch)

"""# **CREATING MODEL WITH BATCH SIZE = 512 , EPOCH = 50**"""

model = Sequential()
# Remark: since coreml cannot accept vector shapes of complex shape like
# [80,3] this workaround is used in order to reshape the vector internally
# prior feeding it into the network
model.add(Reshape((TIME_PERIODS, 3), input_shape=(input_shape,)))
model.add(Dense(150, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(140, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(130, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(120, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(100, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(80, activation='relu'))
model.add(Flatten())
model.add(Dense(num_classes, activation='softmax'))
print(model.summary())

model.compile(loss='categorical_crossentropy',
                optimizer='adam', metrics=['accuracy'])

# Hyper-parameters
BATCH_SIZE = 512
EPOCHS = 50

# Enable validation to use ModelCheckpoint and EarlyStopping callbacks.
history = model.fit(x_train,
                      y_train_hot,
                      batch_size=BATCH_SIZE,
                      epochs=EPOCHS,
                      validation_split=0.2,
                      verbose=1)

"""# **PLOTTING ACCURACY VS EPOCH**"""

plt.figure(figsize=(5, 4))
plt.plot(history.history['accuracy'], 'r-', label='Accuracy of training data')
plt.plot(history.history['val_accuracy'], 'b-', label='Accuracy of validation data')
plt.ylabel('Accuracy')
plt.xlabel('Training Epoch')
plt.title('ACCURACY Vs EPOCH')
plt.legend()
plt.show()

"""# **PLOTTING LOSS VS EPOCH**"""

plt.figure(figsize=(5, 4))
plt.plot(history.history['loss'], 'r-', label='Loss of training data')
plt.plot(history.history['val_loss'], 'y-', label='Loss of validation data')
plt.title('LOSS Vs EPOCH')
plt.ylabel('Loss')
plt.xlabel('Training Epoch')
plt.legend()
plt.show()

"""# **PREDICTING LABELS FOR TEST DATA USING TRAINED MODEL**"""

y_pred_train = model.predict(x_train)
# Take the class with the highest probability from the train predictions
max_y_pred_train = np.argmax(y_pred_train, axis=1)
print(classification_report(y_train, max_y_pred_train))

"""# **CALCULATING ACCURACY OF PREDICTION**"""

score = model_m.evaluate(x_test, y_test, verbose=1)
accuracy_test = score[1] * 100

print('\nLoss on test data: %0.2f' % score[0])
print("\n ACCURACY ON TEST DATA = ", accuracy_test,"%")

"""# **PLOTTING CONFUSION MATRIX**"""

def show_confusion_matrix(validations, predictions):

    matrix = metrics.confusion_matrix(validations, predictions)
    plt.figure(figsize=(6, 4))
    sns.heatmap(matrix,
                cmap='Greens',
                linecolor='white',
                linewidths=1,
                xticklabels=LABELS,
                yticklabels=LABELS,
                annot=True,
                fmt='d')
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

y_pred_test = model_m.predict(x_test)
# Take the class with the highest probability from the test predictions
max_y_pred_test = np.argmax(y_pred_test, axis=1)
max_y_test = np.argmax(y_test, axis=1)

print(classification_report(max_y_test, max_y_pred_test))

show_confusion_matrix(max_y_test, max_y_pred_test)